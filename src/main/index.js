import { app, shell, BrowserWindow, ipcMain, dialog } from 'electron'
import { join, dirname } from 'path'
import { electronApp, optimizer, is } from '@electron-toolkit/utils'
import fs from 'fs-extra'
import { GoogleGenerativeAI } from '@google/generative-ai'
import { exec } from 'child_process'
import { promisify } from 'util'
import Store from 'electron-store'
import log from 'electron-log'
import axios from 'axios'
import { autoUpdater } from 'electron-updater'
import { EdgeTTS } from 'node-edge-tts'
// --- CONSTANTS & CONFIG ---
const IMAGE_API_URL = 'https://voiceapi.csv666.ru/api/v1'
const GENAI_API_URL = 'https://genaipro.vn/api/v1'
const VOICE_API_URL = 'https://voiceapi.csv666.ru'

const execPromise = promisify(exec)
const store = new Store()
const sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms))

// Setup logging for AutoUpdater
autoUpdater.logger = log
autoUpdater.logger.transports.file.level = 'info'
autoUpdater.autoDownload = true
autoUpdater.autoInstallOnAppQuit = true

let mainWindow

function createWindow() {
  mainWindow = new BrowserWindow({
    width: 1200,
    height: 750,
    minWidth: 900,
    minHeight: 600,
    backgroundColor: '#121417',
    show: false,
    frame: true,
    autoHideMenuBar: true,
    ...(process.platform === 'linux' ? { icon: join(__dirname, '../../build/icon.png') } : {}),
    webPreferences: {
      preload: join(__dirname, '../preload/index.js'),
      sandbox: false,
      nodeIntegration: false,
      contextIsolation: true
    }
  })

  mainWindow.on('ready-to-show', () => {
    mainWindow.show()
    setTimeout(() => {
      autoUpdater.checkForUpdatesAndNotify()
    }, 2000)
  })

  mainWindow.webContents.setWindowOpenHandler((details) => {
    shell.openExternal(details.url)
    return { action: 'deny' }
  })

  if (is.dev && process.env['ELECTRON_RENDERER_URL']) {
    mainWindow.loadURL(process.env['ELECTRON_RENDERER_URL'])
  } else {
    mainWindow.loadFile(join(__dirname, '../renderer/index.html'))
  }
}

// --- APP LIFECYCLE ---

app.whenReady().then(() => {
  electronApp.setAppUserModelId('com.storymaker.app')

  app.on('browser-window-created', (_, window) => {
    optimizer.watchWindowShortcuts(window)
  })

  createWindow()

  app.on('activate', function () {
    if (BrowserWindow.getAllWindows().length === 0) createWindow()
  })
})

app.on('window-all-closed', () => {
  if (process.platform !== 'darwin') {
    app.quit()
  }
})

// --- AUTO UPDATER EVENTS ---
autoUpdater.on('checking-for-update', () => sendStatus('checking', 'Checking...'))
autoUpdater.on('update-available', () => sendStatus('downloading', 'Update found. Downloading...'))
autoUpdater.on('download-progress', (progress) => {
  const percent = Math.round(progress.percent)
  if (mainWindow) {
    mainWindow.webContents.send('log-update', `‚¨áÔ∏è Downloading update: ${percent}%`)
  }
})
autoUpdater.on('update-downloaded', () => sendStatus('ready', 'Update Ready!'))
autoUpdater.on('error', (err) => {
  log.error('Update error:', err)
  sendStatus('error', 'Update Error')
})

function sendStatus(state, msg) {
  if (mainWindow) {
    mainWindow.webContents.send('log-update', `‚ÑπÔ∏è Updater: ${msg}`)
  }
}

// --- IPC HANDLERS: SETTINGS & FILES ---
ipcMain.handle('get-setting', (event, key) => store.get(key, null))
ipcMain.handle('save-setting', (event, key, value) => {
  store.set(key, value)
  return true
})

ipcMain.handle('select-folder', async () => {
  const result = await dialog.showOpenDialog(mainWindow, { properties: ['openDirectory'] })
  return result.canceled ? null : result.filePaths[0]
})

ipcMain.handle('select-file', async (event, extensions = []) => {
  const filters =
    extensions.length > 0
      ? [{ name: 'Custom Files', extensions }]
      : [{ name: 'All Files', extensions: ['*'] }]
  const result = await dialog.showOpenDialog(mainWindow, { properties: ['openFile'], filters })
  return result.canceled ? null : result.filePaths[0]
})

ipcMain.handle('read-json', async (e, filePath) => {
  try {
    return await fs.readJson(filePath)
  } catch (error) {
    console.error('JSON Read Error:', error)
    return null
  }
})

ipcMain.handle('write-json', async (e, filePath, data) => {
  try {
    await fs.writeJson(filePath, data, { spaces: 2 })
    return true
  } catch (error) {
    console.error('JSON Write Error:', error)
    return false
  }
})

ipcMain.handle('get-version', () => app.getVersion())

// --- IPC HANDLERS: HISTORY ---
ipcMain.handle('get-history', () => store.get('generationHistory', []))
ipcMain.handle('clear-history', () => {
  store.set('generationHistory', [])
  return true
})
ipcMain.handle('open-folder', async (e, path) => {
  await shell.openPath(path)
})

// --- HELPER FUNCTIONS ---
function formatTimeSRT(seconds) {
  const date = new Date(0)
  date.setMilliseconds(seconds * 1000)
  const hh = date.getUTCHours().toString().padStart(2, '0')
  const mm = date.getUTCMinutes().toString().padStart(2, '0')
  const ss = date.getUTCSeconds().toString().padStart(2, '0')
  const ms = date.getUTCMilliseconds().toString().padStart(3, '0')
  return `${hh}:${mm}:${ss},${ms}`
}
function splitTextSafe(text, maxLength = 2500) {
  // –†–æ–∑–±–∏–≤–∞—î–º–æ –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö (—à—É–∫–∞—î–º–æ –∫—Ä–∞–ø–∫—É, –∑–Ω–∞–∫ –æ–∫–ª–∏–∫—É/–ø–∏—Ç–∞–Ω–Ω—è –∞–±–æ –Ω–æ–≤–∏–π —Ä—è–¥–æ–∫)
  const sentences = text.match(/[^.!?\n]+[.!?\n]+/g) || [text]
  const chunks = []
  let currentChunk = ''

  for (const sentence of sentences) {
    if ((currentChunk + sentence).length > maxLength) {
      chunks.push(currentChunk)
      currentChunk = sentence
    } else {
      currentChunk += sentence
    }
  }
  if (currentChunk) chunks.push(currentChunk)

  return chunks
}
// –§—É–Ω–∫—Ü—ñ—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó SRT —Ñ–∞–π–ª—É —á–µ—Ä–µ–∑ Whisper
async function generateSrtWithWhisper(audioPath, srtPath, languageCode = 'auto') {
  sendLog(`üéôÔ∏è Whisper: Initializing (Lang: ${languageCode})...`)

  try {
    const isDev = !app.isPackaged

    // –í–∏–∑–Ω–∞—á–∞—î–º–æ –Ω–∞–∑–≤—É —Ñ–∞–π–ª—É –∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ –û–°
    // –Ø–∫—â–æ Windows -> whisper.exe, —è–∫—â–æ Mac/Linux -> whisper
    const executableName = process.platform === 'win32' ? 'whisper.exe' : 'whisper'

    // 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π —à–ª—è—Ö
    const defaultBinPath = isDev ? join(__dirname, '../../bin') : join(process.resourcesPath, 'bin')

    // 2. –ö–∞—Å—Ç–æ–º–Ω–∏–π —à–ª—è—Ö (–∑ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω—å)
    const customBinPath = store.get('whisperBinPath')

    let binPath = defaultBinPath

    // 3. –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ —à–ª—è—Ö—É
    if (customBinPath && typeof customBinPath === 'string') {
      const customExe = join(customBinPath, executableName)
      const customModel = join(customBinPath, 'ggml-base.bin')

      if (fs.existsSync(customExe) && fs.existsSync(customModel)) {
        binPath = customBinPath
        sendLog(`‚ÑπÔ∏è Using Custom Whisper Path: ${binPath}`)
      } else {
        sendLog(`‚ö†Ô∏è Custom path invalid. Reverting to default: ${defaultBinPath}`)
      }
    } else {
      sendLog(`‚ÑπÔ∏è Using Default Whisper Path: ${binPath}`)
    }

    const whisperExe = join(binPath, executableName)
    const modelPath = join(binPath, 'ggml-base.bin')

    // –§—ñ–Ω–∞–ª—å–Ω–∞ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞
    if (!fs.existsSync(whisperExe)) throw new Error(`Whisper executable missing at: ${whisperExe}`)
    if (!fs.existsSync(modelPath)) throw new Error(`Model missing at: ${modelPath}`)

    // –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –∞—É–¥—ñ–æ (–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü—ñ—è –≤ 16kHz WAV –±–µ–∑ –º–µ—Ç–∞–¥–∞–Ω–∏—Ö)
    const workDir = dirname(audioPath)
    const tempWavName = 'temp_clean.wav'
    const tempWavPath = join(workDir, tempWavName)

    let ffmpegCmd = store.get('customFfmpegPath') || 'ffmpeg'
    ffmpegCmd = ffmpegCmd.replace(/"/g, '')

    sendLog('üéôÔ∏è Converting audio to 16kHz WAV...')
    const convertCmd = `"${ffmpegCmd}" -y -i "${audioPath}" -ar 16000 -ac 1 -c:a pcm_s16le -map_metadata -1 -fflags +bitexact "${tempWavPath}"`
    await execPromise(convertCmd)

    // –ó–∞–ø—É—Å–∫ Whisper
    const outputBase = 'subtitles'

    // –¢—É—Ç –º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑–Ω–∞–π–¥–µ–Ω—ñ —à–ª—è—Ö–∏
    const runCmd = `"${whisperExe}" -m "${modelPath}" -f "${tempWavName}" -osrt -of "${outputBase}" -l ${languageCode} --max-len 40`

    sendLog('üéôÔ∏è Running Whisper AI (Max-len 60)...')
    await execPromise(runCmd, { cwd: workDir })

    // –ß–∏—Å—Ç–∫–∞ —ñ –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞
    fs.unlink(tempWavPath).catch(() => {})
    const generatedFile = join(workDir, outputBase + '.srt')

    if (fs.existsSync(generatedFile)) {
      if (generatedFile !== srtPath) await fs.move(generatedFile, srtPath, { overwrite: true })
      sendLog('‚úÖ SRT generated successfully.')
      return true
    } else {
      // Check fallback name (—ñ–Ω–æ–¥—ñ –≤—ñ—Å–ø–µ—Ä –Ω–∞–∑–∏–≤–∞—î —Ñ–∞–π–ª —è–∫ –∞—É–¥—ñ–æ—Ñ–∞–π–ª + .srt)
      const weirdFile = join(workDir, tempWavName + '.srt')
      if (fs.existsSync(weirdFile)) {
        await fs.move(weirdFile, srtPath, { overwrite: true })
        return true
      }
      console.warn('Whisper finished but no SRT file found (maybe silence).')
      return false // –ù–µ –∫–∏–¥–∞—î–º–æ –ø–æ–º–∏–ª–∫—É, –ø—Ä–æ—Å—Ç–æ –π–¥–µ–º–æ –¥–∞–ª—ñ –±–µ–∑ —Å—É–±—Ç–∏—Ç—Ä—ñ–≤
    }
  } catch (error) {
    console.error('Whisper Failed:', error)
    sendLog(`‚ö†Ô∏è Whisper Error: ${error.message}`)
    return false
  }
}

// --- –û–ù–û–í–õ–ï–ù–ò–ô –û–ë–†–û–ë–ù–ò–ö GENERATE-AUDIO-ONLY ---

const sendLog = (msg) => {
  if (mainWindow) mainWindow.webContents.send('log-update', msg)
}

// –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ç—Ä–∏–≤–∞–ª–æ—Å—Ç—ñ –∞—É–¥—ñ–æ
async function getAudioDuration(audioPath, ffmpegPath) {
  try {
    let ffprobeCmd = 'ffprobe'
    if (ffmpegPath && ffmpegPath.toLowerCase().includes('ffmpeg')) {
      ffprobeCmd = ffmpegPath.replace(/ffmpeg(?:\.exe)?$/i, 'ffprobe.exe')
    }
    ffprobeCmd = ffprobeCmd.replace(/"/g, '')

    const cmd = `"${ffprobeCmd}" -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${audioPath}"`
    const { stdout } = await execPromise(cmd)
    const duration = parseFloat(stdout.trim())
    return isNaN(duration) ? 300 : duration
  } catch (e) {
    console.warn('FFprobe failed (using default 300s):', e.message)
    return 300
  }
}

async function generateElevenLabsImage(prompt, token, outputPath) {
  try {
    const cleanToken = token.trim()
    const response = await axios.post(
      `${IMAGE_API_URL}/image/create?as_file=true`,
      { prompt: prompt, aspect_ratio: '16:9' },
      {
        headers: {
          'x-api-key': cleanToken,
          'api-key': cleanToken,
          'Content-Type': 'application/json'
        },
        responseType: 'arraybuffer'
      }
    )
    await fs.writeFile(outputPath, response.data)
  } catch (error) {
    console.error('11Labs Error:', error.message)
    throw error
  }
}

async function downloadPollinationsImage(prompt, outputPath) {
  const seed = Math.floor(Math.random() * 1000000)
  const url = `https://image.pollinations.ai/prompt/${encodeURIComponent(
    prompt
  )}?width=1280&height=720&model=flux&seed=${seed}&nologo=true`

  const writer = fs.createWriteStream(outputPath)
  const response = await axios({
    url,
    method: 'GET',
    responseType: 'stream'
  })

  response.data.pipe(writer)
  return new Promise((resolve, reject) => {
    writer.on('finish', resolve)
    writer.on('error', reject)
  })
}
async function generate11LabsAudio(text, voiceId, token, outputPath) {
  sendLog('üéôÔ∏è 11Labs Audio: Creating task...')
  const apiKey = token.trim()

  try {
    const createResponse = await axios.post(
      `${VOICE_API_URL}/tasks`,
      {
        text: text,
        template_uuid: voiceId
      },
      {
        headers: {
          'X-API-Key': apiKey,
          'Content-Type': 'application/json'
        }
      }
    )

    const taskId = createResponse.data.task_id
    if (!taskId) throw new Error('11Labs Audio: No Task ID returned')

    sendLog(`üéôÔ∏è Task started (ID: ${taskId}). Waiting for result...`)

    // 2. –û—á—ñ–∫—É–≤–∞–Ω–Ω—è —Ç–∞ —Å–∫–∞—á—É–≤–∞–Ω–Ω—è
    let attempts = 0
    const maxAttempts = 450 // –ß–µ–∫–∞—î–º–æ –¥–æ 15 —Ö–≤

    while (attempts < maxAttempts) {
      await sleep(2000)

      const statusRes = await axios.get(`${VOICE_API_URL}/tasks/${taskId}/status`, {
        headers: { 'X-API-Key': apiKey }
      })

      const status = statusRes.data.status
      console.log(`[11Labs] Status: ${status} (Attempt ${attempts})`)

      // –Ø–∫—â–æ —Å—Ç–∞—Ç—É—Å ending –∞–±–æ ending_processed ‚Äî –∫–∞—á–∞—î–º–æ
      if (status === 'ending' || status === 'ending_processed') {
        sendLog('üéôÔ∏è Status is ready. Downloading file...')

        const writer = fs.createWriteStream(outputPath)

        const response = await axios({
          method: 'GET',
          url: `${VOICE_API_URL}/tasks/${taskId}/result`,
          headers: { 'X-API-Key': apiKey },
          responseType: 'stream'
        })

        // –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞: —è–∫—â–æ —Å–µ—Ä–≤–µ—Ä —Ä–∞–ø—Ç–æ–º –ø–æ–≤–µ—Ä–Ω—É–≤ JSON –∑ –ø–æ–º–∏–ª–∫–æ—é –∑–∞–º—ñ—Å—Ç—å —Ñ–∞–π–ª—É
        if (
          response.headers['content-type'] &&
          response.headers['content-type'].includes('application/json')
        ) {
          throw new Error(
            'Server returned JSON instead of Audio file (Check Voice ID/Template UUID)'
          )
        }

        response.data.pipe(writer)

        return new Promise((resolve, reject) => {
          writer.on('finish', () => {
            sendLog('‚úÖ Audio downloaded successfully.')
            resolve()
          })
          writer.on('error', (err) => {
            console.error('File Write Error:', err)
            reject(err)
          })
        })
      }

      if (status === 'error') {
        throw new Error('11Labs Audio Task returned Error status from server')
      }

      attempts++
    }

    throw new Error('11Labs Audio: Generation timed out')
  } catch (err) {
    console.error('11Labs Audio API Error:', err.response ? err.response.data : err.message)
    throw new Error(`11Labs Audio Failed: ${err.message}`)
  }
}

async function generateGenAiAudio(text, voiceId, token, outputPath) {
  sendLog('üéôÔ∏è GenAI: Sending text...')
  const createRes = await axios.post(
    `${GENAI_API_URL}/labs/task`,
    { input: text, voice_id: voiceId, model_id: 'eleven_multilingual_v2', speed: 1.0 },
    { headers: { Authorization: `Bearer ${token}`, 'Content-Type': 'application/json' } }
  )

  const taskId = createRes.data.task_id
  if (!taskId) throw new Error('GenAI: Task ID not received')

  let audioUrl = null
  let attempts = 0
  while (attempts < 600) {
    await sleep(2000)
    const statusRes = await axios.get(`${GENAI_API_URL}/labs/task/${taskId}`, {
      headers: { Authorization: `Bearer ${token}` }
    })
    const status = statusRes.data.status
    if (status === 'completed') {
      audioUrl = statusRes.data.result
      break
    } else if (status === 'failed') throw new Error('GenAI Task Failed')
    attempts++
  }
  if (!audioUrl) throw new Error('GenAI: Timeout')

  const writer = fs.createWriteStream(outputPath)
  const response = await axios({ url: audioUrl, method: 'GET', responseType: 'stream' })
  response.data.pipe(writer)
  return new Promise((resolve, reject) => {
    writer.on('finish', resolve)
    writer.on('error', reject)
  })
}
async function addFadeEffectToSrt(srtPath) {
  try {
    // üî• –í–ê–ñ–õ–ò–í–û: –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —ñ—Å–Ω—É—î —Ñ–∞–π–ª –ø–µ—Ä–µ–¥ —Ç–∏–º, —è–∫ –π–æ–≥–æ —á–∏—Ç–∞—Ç–∏
    // –Ø–∫—â–æ —Ñ–∞–π–ª—É –Ω–µ–º–∞—î (–±–æ —Ç–∏ –∑–Ω—è–≤ –≥–∞–ª–æ—á–∫—É), –º–∏ –ø—Ä–æ—Å—Ç–æ –≤–∏—Ö–æ–¥–∏–º–æ –∑ —Ñ—É–Ω–∫—Ü—ñ—ó
    if (!fs.existsSync(srtPath)) {
      return
    }

    let content = await fs.readFile(srtPath, 'utf8')

    // –†–µ–≥—É–ª—è—Ä–Ω–∏–π –≤–∏—Ä–∞–∑ —à—É–∫–∞—î —Ç–µ–∫—Å—Ç —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ —ñ –¥–æ–¥–∞—î —Ç–µ–≥ {\fad(400,0)}
    // –¶–µ –æ–∑–Ω–∞—á–∞—î: –ø–ª–∞–≤–Ω–∞ –ø–æ—è–≤–∞ –∑–∞ 400–º—Å (0.4—Å)
    const lines = content.split('\n')
    const newLines = lines.map((line) => {
      // –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ —Ä—è–¥–∫–∏, –Ω–æ–º–µ—Ä–∏ (—Ü–∏—Ñ—Ä–∏) —ñ —Ç–∞–π–º–∫–æ–¥–∏ (-->)
      if (!line.trim() || /^\d+$/.test(line.trim()) || line.includes('-->')) {
        return line
      }

      // –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —â–æ–± –Ω–µ –¥–æ–¥–∞–≤–∞—Ç–∏ —Ç–µ–≥ –¥–≤—ñ—á—ñ
      if (line.includes('{\\fad')) return line

      // –î–æ–¥–∞—î–º–æ —Ç–µ–≥ –ø–µ—Ä–µ–¥ —Ç–µ–∫—Å—Ç–æ–º
      return `{\\fad(400,0)}${line}`
    })

    await fs.writeFile(srtPath, newLines.join('\n'), 'utf8')
    console.log('‚úÖ Animation tags added to SRT.')
  } catch (e) {
    // –¢–µ–ø–µ—Ä –ø–æ–º–∏–ª–∫–∞ ENOENT –Ω–µ –ø–æ–≤–∏–Ω–Ω–∞ –∑'—è–≤–ª—è—Ç–∏—Å—è, –∞–ª–µ –ª–æ–≥ –∑–∞–ª–∏—à–∞—î–º–æ
    console.error('Failed to add fade effects:', e)
  }
}

// --- –û–ù–û–í–õ–ï–ù–ê –§–£–ù–ö–¶–Ü–Ø createVideoFromProject ---
async function createVideoFromProject(folderPath, visualMode = 'images') {
  try {
    const audioName = 'audio.mp3'
    const videoName = 'video.mp4'
    const srtName = 'subtitles.srt'

    const audioPath = join(folderPath, audioName)
    const srtPath = join(folderPath, srtName)

    let ffmpegCmd = store.get('customFfmpegPath') || 'ffmpeg'
    ffmpegCmd = ffmpegCmd.replace(/"/g, '')

    const subSettings = store.get('subtitleSettings') || {
      font: 'Merriweather Light',
      size: 24,
      primary: '#FFFFFF',
      outline: '#000000',
      borderStyle: '1',
      alignment: '2',
      italic: true,
      outlineWidth: 0
    }
    const fontName = subSettings.font
    const fontSize = subSettings.size
    const outlineWidth = subSettings.outlineWidth || 0
    const assPrimary = hexToAssColor(subSettings.primary)
    const assOutline = hexToAssColor(subSettings.outline)
    const borderStyle = subSettings.borderStyle
    const alignment = subSettings.alignment
    const italic = subSettings.italic ? '1' : '0'

    const styleASS = `Fontname=${fontName},Italic=${italic},Fontsize=${fontSize},PrimaryColour=${assPrimary},OutlineColour=${assOutline},BorderStyle=${borderStyle},Outline=${outlineWidth},Shadow=0.5,MarginV=25,Alignment=${alignment}`

    // 1. Get Audio Duration
    sendLog('üé¨ Analyzing audio length...')
    const audioDuration = await getAudioDuration(audioPath, ffmpegCmd)
    sendLog(`‚ÑπÔ∏è Audio Duration: ${audioDuration}s`)

    // –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ—ñ–ª—å—Ç—Ä–∞ —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ (UTF-8)
    let subtitlesFilter = ''
    if (fs.existsSync(srtPath)) {
      // –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ :charenc=UTF-8 —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –∫—Ä—è–∫–æ–∑—è–±—Ä—ñ–≤
      // –í—ñ–¥–Ω–æ—Å–Ω–∏–π —à–ª—è—Ö srtName –ø—Ä–∞—Ü—é—î –∫—Ä–∞—â–µ –∑ execOptions.cwd
      subtitlesFilter = `,subtitles='${srtName}':charenc=UTF-8:force_style='${styleASS}'`
    }

    const execOptions = { cwd: folderPath }

    // ============================
    // –†–ï–ñ–ò–ú 1: –í–Ü–î–ï–û-–õ–£–ü
    // ============================
    if (visualMode === 'video') {
      const bgVideo = 'source_bg.mp4'
      if (!fs.existsSync(join(folderPath, bgVideo))) throw new Error('Source video missing!')

      sendLog('üé¨ Rendering looped video with subtitles...')

      // scale=1920:1080...crop... -> –†–æ–±–∏–º–æ 16:9 —ñ –∑–∞–ø–æ–≤–Ω—é—î–º–æ –µ–∫—Ä–∞–Ω
      const filter = `scale=1920:1080:force_original_aspect_ratio=increase,crop=1920:1080,setsar=1${subtitlesFilter}`

      // -stream_loop -1: –ù–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–∏–π –ø–æ–≤—Ç–æ—Ä –≤—ñ–¥–µ–æ
      // -shortest: –û–±—Ä—ñ–∑–∞—Ç–∏ –ø–æ –Ω–∞–π–∫–æ—Ä–æ—Ç—à–æ–º—É (–ø–æ –∞—É–¥—ñ–æ)
      const command = `"${ffmpegCmd}" -y -stream_loop -1 -i "${bgVideo}" -i "${audioName}" -vf "${filter}" -map 0:v -map 1:a -c:v libx264 -preset medium -crf 18 -c:a aac -b:a 192k -shortest "${videoName}"`
      await execPromise(command, execOptions)
    }
    // ============================
    // –†–ï–ñ–ò–ú 2: –ö–ê–†–¢–ò–ù–ö–ò (–°–õ–ê–ô–î-–®–û–£)
    // ============================
    else {
      const imagesDir = join(folderPath, 'images')
      if (!fs.existsSync(imagesDir)) throw new Error('Images folder missing!')

      const files = await fs.readdir(imagesDir)
      const uniqueImages = files
        .filter((f) => f.endsWith('.jpg') || f.endsWith('.png'))
        .sort((a, b) => (parseInt(a.match(/\d+/)) || 0) - (parseInt(b.match(/\d+/)) || 0))

      if (uniqueImages.length === 0) throw new Error('No images found!')

      sendLog(`üé¨ Found ${uniqueImages.length} images for video.`)

      // ‚ùå –í–ò–î–ê–õ–ï–ù–û: const style = 'Fontname=...' (–±–æ –º–∏ –≤–∂–µ –º–∞—î–º–æ styleASS –∑–≤–µ—Ä—Ö—É)

      if (uniqueImages.length === 1) {
        // –û–¥–Ω–µ —Ñ–æ—Ç–æ
        const relImgPath = `images/${uniqueImages[0]}`

        let filter = 'format=yuv420p'
        if (fs.existsSync(srtPath)) {
          // ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: –∑–∞–º—ñ—Å—Ç—å style —Å—Ç–∞–≤–∏–º–æ styleASS
          filter += `,subtitles='${srtName}':charenc=UTF-8:force_style='${styleASS}'`
        }

        const command = `"${ffmpegCmd}" -y -loop 1 -i "${relImgPath}" -i "${audioName}" -vf "${filter}" -c:v libx264 -preset medium -crf 18 -tune stillimage -c:a aac -b:a 192k -shortest "${videoName}"`
        await execPromise(command, execOptions)
      } else {
        // –°–ª–∞–π–¥-—à–æ—É
        const slideDuration = 20
        const fadeDuration = 1
        const effectiveSlideTime = slideDuration - fadeDuration
        const totalSlidesNeeded = Math.ceil(audioDuration / effectiveSlideTime) + 1

        let inputFilesList = []
        for (let i = 0; i < totalSlidesNeeded; i++) {
          const imgIndex = i % uniqueImages.length
          inputFilesList.push(`images/${uniqueImages[imgIndex]}`)
        }

        let inputs = ''
        inputFilesList.forEach((p) => {
          inputs += `-loop 1 -t ${slideDuration} -i "${p}" `
        })

        let filter = ''
        let lastLabel = '[0:v]'
        let offset = slideDuration - fadeDuration

        for (let i = 1; i < inputFilesList.length; i++) {
          const nextLabel = `[${i}:v]`
          const outLabel = `[v${i}]`
          filter += `${lastLabel}${nextLabel}xfade=transition=fade:duration=${fadeDuration}:offset=${offset}${outLabel};`
          lastLabel = outLabel
          offset += slideDuration - fadeDuration
        }

        if (fs.existsSync(srtPath)) {
          // ‚úÖ –í–ò–ü–†–ê–í–õ–ï–ù–û: –∑–∞–º—ñ—Å—Ç—å style —Å—Ç–∞–≤–∏–º–æ styleASS
          filter += `${lastLabel}format=yuv420p[v_pre];[v_pre]subtitles='${srtName}':charenc=UTF-8:force_style='${styleASS}'[v]`
        } else {
          filter += `${lastLabel}format=yuv420p[v]`
        }

        const command = `"${ffmpegCmd}" -y ${inputs} -i "${audioName}" -filter_complex "${filter}" -map "[v]" -map ${inputFilesList.length}:a -c:v libx264 -preset medium -crf 18 -c:a aac -b:a 192k -shortest "${videoName}"`
        await execPromise(command, execOptions)
      }
    }

    sendLog('üöÄ Video Rendered Successfully: video.mp4')
  } catch (err) {
    sendLog(`‚ö†Ô∏è Video Render Error: ${err.message}`)
    console.error(err)
  }
}

// --- IPC HANDLERS: GENERATION FLOW ---

ipcMain.handle('generate-story-text', async (event, data) => {
  const {
    projectName,
    storyPrompt,
    seoPrompt,
    title,
    language,
    outputFolder,
    modelName,
    targetLength,
    onePartStory
  } = data

  try {
    const apiKey = store.get('apiKey')
    if (!apiKey) throw new Error('Gemini API Key is missing.')

    // 1. –ü–ï–†–ï–í–Ü–†–ö–ê
    if (!storyPrompt || typeof storyPrompt !== 'string') {
      throw new Error('Template (storyPrompt) is missing! Check frontend.')
    }

    const genAI = new GoogleGenerativeAI(apiKey)
    const selectedModel = modelName || 'gemini-2.0-flash'
    const model = genAI.getGenerativeModel({ model: selectedModel })

    // 2. –ü–ê–ü–ö–ò
    const safeProjectName = projectName
      .replace(/[–∞-—è–ê-–Ø—ñ–Ü—ó–á—î–Ñ“ë“ê]/g, 'ua')
      .replace(/[^a-zA-Z0-9]/g, '_')

    const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19)
    const folderName = `${safeProjectName}_${timestamp}`
    const finalPath = join(outputFolder, folderName)

    await fs.ensureDir(finalPath)

    sendLog('‚úçÔ∏è Starting Story Generation...')
    console.log('Raw targetLength:', targetLength)
    // 3. –ü–Ü–î–ì–û–¢–û–í–ö–ê –ü–†–û–ú–ü–¢–£
    let finalInitialPrompt = storyPrompt
      .replace(/{title}/gi, title)
      .replace(/{language}/gi, language)
      .replace(/{length}/gi, targetLength || '25000')
      .replace(/{projectName}/gi, projectName)

    // üî• –ü–û–ö–†–ê–©–ï–ù–Ü –ü–†–ê–í–ò–õ–ê (–í–∏–±—ñ—Ä —Ä–µ–∂–∏–º—É)
    let systemRules = ''

    if (onePartStory) {
      // === –ü–†–ê–í–ò–õ–ê –î–õ–Ø –û–î–ù–Ü–Ñ–á –ß–ê–°–¢–ò–ù–ò ===
      systemRules = `
        \n\nSYSTEM RULES (MUST FOLLOW):
        1. Write the COMPLETE story in ONE SINGLE RESPONSE.
        2. Write a full, finished story from start to end.
        3. CRITICAL: WRITE ONLY IN THIS LANGUAGE: ${language}.
        4. No markdown headers (like # Chapter 1).
      `
    } else {
      // === –°–¢–ê–†–Ü –ü–†–ê–í–ò–õ–ê (–ß–ê–°–¢–ò–ù–ê–ú–ò) ===
      systemRules = `
        \n\nSYSTEM RULES (MUST FOLLOW):
        1. Write the story in parts. Do NOT write the whole story at once.
        2. At the end of a part, write exactly "CONTINUE" if not finished.
        3. If the story is completely finished, write exactly "END".
        4. CRITICAL: WRITE THE STORY ONLY IN THIS LANGUAGE: ${language}.
        5. No markdown headers (like # Chapter 1).
      `
    }

    // –ü–µ—Ä—à–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
    let nextMessage = finalInitialPrompt + systemRules

    const chat = model.startChat({ history: [] })
    let fullStoryText = ''
    let isFinished = false
    let iteration = 0

    // 4. –¶–ò–ö–õ –ì–ï–ù–ï–†–ê–¶–Ü–á
    while (!isFinished && iteration < 70) {
      iteration++
      sendLog(`‚úçÔ∏è Writing part ${iteration} (Lang: ${language})...`)

      console.log(`\nüîµ === AI PROMPT (Iteration ${iteration}) ===`)
      console.log(nextMessage) // –í–∏–≤–æ–¥–∏—Ç—å –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –ø—Ä–æ–º–ø—Ç—É
      console.log('===========================================\n')

      try {
        const result = await chat.sendMessage(nextMessage)
        const rawText = result.response.text()

        // üî• –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–£
        let cleanChunk = rawText
          .replace(/CONTINUE/gi, '')
          .replace(/END/gi, '')
          .replace(/Type .*? to receive the next part\.?/gi, '')
          .replace(/Type .*? to continue\.?/gi, '')
          .replace(/\(Write .*?\)/gi, '')
          .replace(/\*\*/g, '')
          .replace(/##/g, '')
          .trim()

        if (cleanChunk) {
          fullStoryText += cleanChunk + '\n\n'
        }

        // --- –£–ú–û–í–ê –í–ò–•–û–î–£ ---
        if (onePartStory) {
          // –õ–æ–≥—ñ–∫–∞ –¥–ª—è "One Part Story"
          // –Ø–∫—â–æ AI –Ω–µ –Ω–∞–ø–∏—Å–∞–≤ —è–≤–Ω–æ "CONTINUE", –º–∏ –≤–≤–∞–∂–∞—î–º–æ, —â–æ –≤—ñ–Ω –∑–∞–∫—ñ–Ω—á–∏–≤
          if (!rawText.includes('CONTINUE')) {
            isFinished = true
            sendLog('‚úÖ One-part story finished.')
          }
        } else {
          // –°—Ç–∞—Ä–∞ –ª–æ–≥—ñ–∫–∞ (Mult-part)
          // –®—É–∫–∞—î–º–æ —Å–ª–æ–≤–æ "END"
          if (rawText.includes('END')) {
            isFinished = true
            sendLog('‚úÖ Story finished by AI.')
          }
        }

        // --- –ü–Ü–î–ì–û–¢–û–í–ö–ê –ù–ê–°–¢–£–ü–ù–û–ì–û –ö–†–û–ö–£ (–Ø–ö–©–û –ù–ï –ó–ê–ö–Ü–ù–ß–ò–õ–ò) ---
        if (!isFinished) {
          nextMessage = `
            Great. Now write the NEXT part of the story. 
            - Move the plot forward. 
            - Do NOT repeat scenes.
            - Keep using language: ${language}.
            (Remember: do not write the end until the story is fully resolved)
          `
          await sleep(2000)
        }
      } catch (err) {
        console.error(`Generation Error at part ${iteration}:`, err)
        break
      }
    }

    // 5. –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø
    const finalContent = fullStoryText.trim()
    if (!finalContent) throw new Error('AI produced empty text.')

    await fs.writeFile(join(finalPath, 'story.txt'), finalContent)

    // 6. SEO
    sendLog('üìù Generating SEO...')
    try {
      const seoTemplate =
        seoPrompt ||
        `Based on the story above, write YouTube Title, Description, Hashtags. Language: ${language}.`

      const finalSeoPrompt = seoTemplate.replace(/{title}/gi, title)

      const descRes = await chat.sendMessage(finalSeoPrompt)
      await fs.writeFile(join(finalPath, 'description.txt'), descRes.response.text().trim())
    } catch (e) {
      console.warn('SEO gen failed', e)
    }

    // 7. –Ü–°–¢–û–†–Ü–Ø
    const history = store.get('generationHistory', [])
    history.unshift({
      title: projectName,
      projectName,
      path: finalPath,
      date: new Date().toLocaleString()
    })
    store.set('generationHistory', history.slice(0, 50))

    return { success: true, textToSpeak: finalContent, folderPath: finalPath }
  } catch (error) {
    console.error('Story Gen Error:', error)
    return { success: false, error: error.message }
  }
})

// STAGE 2: Audio, Images, Video
// --- IPC HANDLER: GENERATE AUDIO ONLY (UPDATED) ---

// –ú–∞–ø–∞ –º–æ–≤ –¥–ª—è Whisper (–î–æ–¥–∞–π —Ü–µ –ø–µ—Ä–µ–¥ —Ñ—É–Ω–∫—Ü—ñ—î—é –∞–±–æ –Ω–∞ –ø–æ—á–∞—Ç–∫—É —Ñ–∞–π–ª—É)
const LANGUAGE_CODES = {
  English: 'en',
  Ukrainian: 'uk',
  German: 'de',
  Spanish: 'es',
  French: 'fr'
  // –ú–æ–∂–µ—à –¥–æ–¥–∞—Ç–∏ —ñ–Ω—à—ñ –º–æ–≤–∏, —è–∫—â–æ –≤–æ–Ω–∏ —î –≤ —Ç–≤–æ—î–º—É select
}

ipcMain.handle('generate-audio-only', async (event, data) => {
  // –î–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü—ñ—è –∑ –Ω–æ–≤–∏–º–∏ –ø–æ–ª—è–º–∏: visualMode, bgVideoPath, language
  const {
    text,
    voice,
    ttsProvider,
    folderPath,
    imagePrompt,
    imageCount,
    visualMode,
    bgVideoPath,
    language,
    makeSubtitles
  } = data

  try {
    // –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç —Å–∫—Ä–∏–ø—Ç–∞
    await fs.writeFile(join(folderPath, 'final_script_for_audio.txt'), text)

    // ==========================================
    // –ö–†–û–ö 1: –í–Ü–ó–£–ê–õ–¨–ù–ò–ô –ö–û–ù–¢–ï–ù–¢ (–ö–ê–†–¢–ò–ù–ö–ò –ê–ë–û –í–Ü–î–ï–û)
    // ==========================================

    if (visualMode === 'video') {
      // --- –†–ï–ñ–ò–ú –í–Ü–î–ï–û ---
      sendLog('üé¨ Video Mode Selected. Skipping image generation.')

      if (!bgVideoPath) {
        throw new Error('Background video file not selected!')
      }
      if (!fs.existsSync(bgVideoPath)) {
        throw new Error(`Video file not found at: ${bgVideoPath}`)
      }

      // –ö–æ–ø—ñ—é—î–º–æ –≤—ñ–¥–µ–æ —É –ø–∞–ø–∫—É –ø—Ä–æ—î–∫—Ç—É —è–∫ "source_bg.mp4"
      const destVideoPath = join(folderPath, 'source_bg.mp4')
      sendLog(`üìÇ Copying background video to project folder...`)
      await fs.copy(bgVideoPath, destVideoPath)
    } else {
      // --- –†–ï–ñ–ò–ú –ö–ê–†–¢–ò–ù–û–ö (–°—Ç–∞—Ä–∞ –ª–æ–≥—ñ–∫–∞) ---
      const imagesDir = join(folderPath, 'images')
      await fs.ensureDir(imagesDir)

      let countToGen = parseInt(imageCount)
      if (isNaN(countToGen) || countToGen < 1) countToGen = 1

      const finalImagePrompt = imagePrompt || 'Atmospheric cinematic background, 8k, detailed'

      sendLog(`üé® Starting Image Generation: Count=${countToGen}...`)

      const imgProvider = store.get('imageProvider') || 'free'
      const imgToken = store.get('elevenLabsImgKey')

      for (let i = 1; i <= countToGen; i++) {
        const imgName = `scene_${i}.jpg`
        const imgPath = join(imagesDir, imgName)

        sendLog(`üé® Generating Image ${i}/${countToGen}...`)

        try {
          if (imgProvider === 'eleven') {
            await generateElevenLabsImage(finalImagePrompt, imgToken, imgPath)
          } else {
            await downloadPollinationsImage(finalImagePrompt, imgPath)
          }
          sendLog(`‚úÖ Image ${i} saved.`)
        } catch (e) {
          console.error(`Failed to generate image ${i}:`, e)
          sendLog(`‚ö†Ô∏è Image ${i} failed. Skipping.`)
        }
        await sleep(1000) // –ü–∞—É–∑–∞ –º—ñ–∂ –∑–∞–ø–∏—Ç–∞–º–∏
      }

      // –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞, —á–∏ —Å—Ç–≤–æ—Ä–∏–ª–∏—Å—å –∫–∞—Ä—Ç–∏–Ω–∫–∏
      const files = await fs.readdir(imagesDir)
      if (files.filter((f) => f.endsWith('.jpg')).length === 0) {
        sendLog('‚ö†Ô∏è WARNING: No images generated! Creating a dummy image...')
        // –¢—É—Ç –º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ –ª–æ–≥—ñ–∫—É —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –∑–∞–≥–ª—É—à–∫–∏, —è–∫—â–æ —Ç—Ä–µ–±–∞
      }
    }

    // ==========================================
    // –ö–†–û–ö 2: –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ê–£–î–Ü–û
    // ==========================================
    const audioPath = join(folderPath, 'audio.mp3')

    if (ttsProvider === 'genai') {
      const gToken = store.get('genAiKey')
      if (!gToken) throw new Error('GenAI Token missing!')
      await generateGenAiAudio(text, voice, gToken, audioPath)
    } else if (ttsProvider === '11labs') {
      const eToken = store.get('elevenAudioKey')
      if (!eToken) throw new Error('11 Labs Audio Key is missing!')
      await generate11LabsAudio(text, voice, eToken, audioPath)
    } else {
      // --- NODE-EDGE-TTS (–ó –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –¥–æ–≤–≥–∏—Ö —Ç–µ–∫—Å—Ç—ñ–≤) ---
      sendLog('üéôÔ∏è Generating Edge TTS Audio (Long Text Mode)...')

      try {
        // 1. –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
        const tts = new EdgeTTS({
          voice: voice,
          lang: 'en-US',
          outputFormat: 'audio-24khz-48kbitrate-mono-mp3',
          timeout: 60000 // 1 —Ö–≤–∏–ª–∏–Ω–∞ –Ω–∞ –∫–æ–∂–µ–Ω –º–∞–ª–µ–Ω—å–∫–∏–π —à–º–∞—Ç–æ–∫ (—Ü—å–æ–≥–æ –¥–æ—Å—Ç–∞—Ç–Ω—å–æ)
        })

        // 2. –†–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
        const chunks = splitTextSafe(text, 2500)
        const totalChunks = chunks.length
        sendLog(`‚ÑπÔ∏è Text split into ${totalChunks} parts. Starting generation...`)

        // 3. –û—á–∏—â–∞—î–º–æ (–∞–±–æ —Å—Ç–≤–æ—Ä—é—î–º–æ) —Ñ—ñ–Ω–∞–ª—å–Ω–∏–π —Ñ–∞–π–ª
        await fs.writeFile(audioPath, '') // –°—Ç–≤–æ—Ä—é—î–º–æ –ø—É—Å—Ç–∏–π —Ñ–∞–π–ª

        // 4. –¶–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó
        for (let i = 0; i < totalChunks; i++) {
          const chunk = chunks[i]
          const tempChunkPath = join(folderPath, `temp_part_${i}.mp3`)

          sendLog(`üéôÔ∏è Processing part ${i + 1}/${totalChunks}...`)

          // –ì–µ–Ω–µ—Ä—É—î–º–æ —à–º–∞—Ç–æ–∫ —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª
          await tts.ttsPromise(chunk, tempChunkPath)

          // –ß–∏—Ç–∞—î–º–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π —à–º–∞—Ç–æ–∫ —ñ –¥–æ–ø–∏—Å—É—î–º–æ –≤ –∫—ñ–Ω–µ—Ü—å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª—É
          const chunkData = await fs.readFile(tempChunkPath)
          await fs.appendFile(audioPath, chunkData)

          // –í–∏–¥–∞–ª—è—î–º–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª, —â–æ–± –Ω–µ —Å–º—ñ—Ç–∏—Ç–∏
          await fs.unlink(tempChunkPath).catch(() => {})

          // –ú–∞–ª–µ–Ω—å–∫–∞ –ø–∞—É–∑–∞, —â–æ–± –Ω–µ "–¥—É—à–∏—Ç–∏" —Å–µ—Ä–≤–µ—Ä Microsoft –∑–∞–ø–∏—Ç–∞–º–∏
          await sleep(500)
        }

        sendLog('‚úÖ Full Edge TTS Audio generated successfully.')
      } catch (e) {
        console.error('NodeEdgeTTS Error:', e)
        throw new Error(`Edge TTS failed at some part: ${e.message}`)
      }
    }

    // ==========================================
    // –ö–†–û–ö 3: –°–£–ë–¢–ò–¢–†–ò (WHISPER)
    // ==========================================
    const srtPath = join(folderPath, 'subtitles.srt')

    // Check the checkbox value
    if (makeSubtitles === true) {
      sendLog('üìù Generating Subtitles with Whisper (Local)...')
      const whisperLangCode = LANGUAGE_CODES[language] || 'auto'

      const srtGenerated = await generateSrtWithWhisper(audioPath, srtPath, whisperLangCode)

      if (srtGenerated) {
        // Only add effects if SRT was actually created
        await addFadeEffectToSrt(srtPath)
      }
    } else {
      sendLog('‚è≠Ô∏è Skipping Subtitles (User unchecked).')
      if (fs.existsSync(srtPath)) {
        await fs.unlink(srtPath)
      }
    }

    // ==========================================
    // –ö–†–û–ö 4: –†–ï–ù–î–ï–† –í–Ü–î–ï–û
    // ==========================================
    // –ü–µ—Ä–µ–¥–∞—î–º–æ visualMode, —â–æ–± —Ñ—É–Ω–∫—Ü—ñ—è –∑–Ω–∞–ª–∞, —â–æ —Ä–µ–Ω–¥–µ—Ä–∏—Ç–∏ (–ª—É–ø —á–∏ —Å–ª–∞–π–¥—à–æ—É)
    await createVideoFromProject(folderPath, visualMode)

    sendLog('‚úÖ All processes completed!')
    shell.openPath(folderPath)
    return { success: true }
  } catch (error) {
    console.error('Stage 2 Error:', error)
    return { success: false, error: error.message }
  }
})
function hexToAssColor(hex) {
  if (!hex) return '&H00FFFFFF'
  const clean = hex.replace('#', '')
  const r = clean.substring(0, 2)
  const g = clean.substring(2, 4)
  const b = clean.substring(4, 6)
  return `&H00${b}${g}${r}`.toUpperCase()
}
